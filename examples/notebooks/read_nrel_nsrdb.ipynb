{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDF5 data in the cloud\n",
    "\n",
    "Many large data collections are hosted in the cloud and are freely availble. \n",
    "E.g.: See https://registry.opendata.aws/\n",
    "\n",
    "For HDF5 data stored in AWS S3, these can directly be accessed with h5py and s3fs, or\n",
    "using HSDS (Highly Scalable Data Service) and h5pyd (h5py-api compatible package for HSDS).\n",
    "\n",
    "This notebook illustrates accessing the NREL NSRDB (National Solar Radiation Database) using both h5pyd \n",
    "and h5py.\n",
    "\n",
    "By running this notebook in codespaces, data access will generally be faster, since the bulk of\n",
    "the data transfer happens on a high-speed internet backbone.  The data is physically located in\n",
    "the AWS us-west-2 region, so speed might be somewhat faster if you select us-west when creating\n",
    "the codespace.\n",
    "\n",
    "Once the codespace environment is ready, you can start evaluating the Jupyter notebooks \n",
    "(by placing the cursor into a code cell and pressing `Ctrl+Enter` or `Shift+Enter`). \n",
    "When prompted for a Python kernel, select\n",
    "\n",
    "```\n",
    "hdf5-tutorial (Python 3.11.x) /opt/conda/envs/hdf5-tutorial/python\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "USE_H5PY = False  # set to True to use h5py/hdf5lib instead\n",
    "if USE_H5PY:\n",
    "    import h5py\n",
    "    import s3fs  # This package enables h5py to \"see\" S3 files as read-only posix files\n",
    "else:\n",
    "    import h5pyd as h5py  # Use the \"as\" syntax for code  compatibility\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hsls is an h5pyd utility to list HSDS domains\n",
    "# In the shell, use the --bucket option to list files from NREL's S3 bucket \n",
    "# run with \"-r\" option to see all domains\n",
    "! hsls --bucket s3://nrel-pds-hsds /nrel/nsrdb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drill down to the conus directory.  Use -H and -v options to show the file sizes\n",
    "# Downloading one of these files would take over a month with a standard\n",
    "# broadband internet connection!\n",
    "\n",
    "! hsls -H -v --bucket s3://nrel-pds-hsds /nrel/nsrdb/conus/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Open one of the nsrdb files.  Use the bucket param to get the data from NREL's S3 bucket\n",
    "if USE_H5PY:\n",
    "    s3 = s3fs.S3FileSystem(anon=True)\n",
    "    f = h5py.File(s3.open(\"s3://nrel-pds-nsrdb/conus/nsrdb_conus_pv_2022.h5\", \"rb\"), \"r\")\n",
    "else:\n",
    "    f = h5py.File(\"/nrel/nsrdb/conus/nsrdb_conus_2022.h5\", bucket=\"s3://nrel-pds-hsds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attributes can be used to provide desriptions of the content\n",
    "%time f.attrs['version']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(f)  # datasets under root group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f[\"air_temperature\"]\n",
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each dataset has an id\n",
    "dset.id.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.shape  # two-dimensional  time x station_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the chunk shape\n",
    "dset.chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the number of bytes per chunk (about 2mb)\n",
    "np.prod(dset.chunks) * dset.dtype.itemsize   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the number of chunks in the dataset\n",
    "(dset.shape[0] // dset.chunks[0]) * (dset.shape[1] // dset.chunks[1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read one year of measurments for a given station index.\n",
    "# this will require reading ~`100MB from S3`\n",
    "%time tseries = dset[::,1234567]\n",
    "tseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get min, max, and mean values\n",
    "tseries.min(), tseries.max(), tseries.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "x = range(len(tseries))\n",
    "plt.plot(x, tseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset is actually linked from an HDF5 file in a different bucket\n",
    "if USE_H5PY:\n",
    "    # this property doesn't exist for h5py\n",
    "    layout = None\n",
    "else:\n",
    "    layout = dset.id.layout\n",
    "layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The HSDS domain actually maps to several different HDF5 files\n",
    "# compile a list of all the files\n",
    "hdf5_files = set()\n",
    "if not USE_H5PY:\n",
    "    for k in f:\n",
    "        dset = f[k]\n",
    "        layout = dset.id.layout\n",
    "        if \"file_uri\" in layout:\n",
    "            hdf5_files.add(layout[\"file_uri\"])\n",
    "hdf5_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
